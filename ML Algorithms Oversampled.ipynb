{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = pd.read_csv('heart_2020_cleaned.csv')\n",
    "dataset.head()\n",
    "\n",
    "# number of data in the dataset\n",
    "print(\"Number of data in the dataset: \", len(dataset.index))\n",
    "print(dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.duplicated().any():      # checking for duplicate data\n",
    "    dataset.drop_duplicates(inplace=True)       # removing the duplicates\n",
    "    print(\"Number of data after removing duplicates: \", dataset.shape[0])\n",
    "else: \n",
    "    print(\"No duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.isnull().values.any():    # checking for null data\n",
    "    print(dataset.isnull().sum())\n",
    "    dataset.dropna()        # removing the null values\n",
    "    print(\"Number of data after removing null values: \", dataset.shape[0])\n",
    "else:\n",
    "    print(\"No null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_target_data = dataset['HeartDisease'].value_counts()      # checking the number of 'yes' and 'no' in the label\n",
    "print(no_of_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We haven't encoded BMI, sleeptime, MentalHealth and PhysicalHealth\n",
    "# They are already integers and floats\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(dataset['HeartDisease'])\n",
    "dataset['HeartDisease']=label_encoder.transform(dataset['HeartDisease'])\n",
    "\n",
    "label_encoder.fit(dataset['Smoking'])\n",
    "dataset['Smoking']=label_encoder.transform(dataset['Smoking'])\n",
    "\n",
    "label_encoder.fit(dataset['AlcoholDrinking'])\n",
    "dataset['AlcoholDrinking']=label_encoder.transform(dataset['AlcoholDrinking'])\n",
    "\n",
    "label_encoder.fit(dataset['Stroke'])\n",
    "dataset['Stroke']=label_encoder.transform(dataset['Stroke'])\n",
    "\n",
    "label_encoder.fit(dataset['DiffWalking'])\n",
    "dataset['DiffWalking']=label_encoder.transform(dataset['DiffWalking'])\n",
    "\n",
    "label_encoder.fit(dataset['Sex'])\n",
    "dataset['Sex']=label_encoder.transform(dataset['Sex'])\n",
    "\n",
    "label_encoder.fit(dataset['AgeCategory'])\n",
    "dataset['AgeCategory']=label_encoder.transform(dataset['AgeCategory'])\n",
    "\n",
    "label_encoder.fit(dataset['Race'])\n",
    "dataset['Race']=label_encoder.transform(dataset['Race'])\n",
    "\n",
    "label_encoder.fit(dataset['Diabetic'])\n",
    "dataset['Diabetic']=label_encoder.transform(dataset['Diabetic'])\n",
    "\n",
    "label_encoder.fit(dataset['PhysicalActivity'])\n",
    "dataset['PhysicalActivity']=label_encoder.transform(dataset['PhysicalActivity'])\n",
    "\n",
    "label_encoder.fit(dataset['GenHealth'])\n",
    "dataset['GenHealth']=label_encoder.transform(dataset['GenHealth'])\n",
    "\n",
    "label_encoder.fit(dataset['Asthma'])\n",
    "dataset['Asthma']=label_encoder.transform(dataset['Asthma'])\n",
    "\n",
    "label_encoder.fit(dataset['KidneyDisease'])\n",
    "dataset['KidneyDisease']=label_encoder.transform(dataset['KidneyDisease'])\n",
    "\n",
    "label_encoder.fit(dataset['SkinCancer'])\n",
    "dataset['SkinCancer']=label_encoder.transform(dataset['SkinCancer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset after encoding the labels\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize= (13,10))\n",
    "sns.heatmap(dataset.corr(),annot = True,cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining features and label\n",
    "x = dataset.drop([\"HeartDisease\"], axis = 1).values     # feature\n",
    "y = dataset[\"HeartDisease\"].values      # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data scaling\n",
    "# robust_scaler = RobustScaler()\n",
    "\n",
    "# x_train = robust_scaler.fit_transform(x_train)\n",
    "# x_test = robust_scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing RandomForest by Handling Imbalanced Dataset using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "counter_before = Counter(y)\n",
    "print(\"Before oversampling: No. of data = \", counter_before)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x1, y1 = smote.fit_resample(x, y)\n",
    "\n",
    "counter_after = Counter(y1)\n",
    "print(\"After oversampling: No. of data = \", counter_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the oversampled dataset\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Total number of data: \", x1.shape[0])\n",
    "print(\"Total number of train data: \", x1_train.shape[0])\n",
    "print(\"Total number of test data: \", x1_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "d_tree1 = tree.DecisionTreeClassifier()\n",
    "d_tree1.fit(x1_train, y1_train)\n",
    "d_tree_pred = d_tree1.predict(x1_test)\n",
    "\n",
    "accuracy = accuracy_score(y1_test, d_tree_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y1_test, d_tree_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y1_test, d_tree_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y1_test, d_tree_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y1_test, d_tree_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y1_test, d_tree_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, d_tree_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier1 = RandomForestClassifier()\n",
    "classifier1.fit(x1_train, y1_train)\n",
    "\n",
    "pred2 = classifier1.predict(x1_test)\n",
    "\n",
    "accuracy1 = accuracy_score(y1_test, pred2)\n",
    "print(\"Accuracy: \", accuracy1*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y1_test, pred2)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y1_test, pred2)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y1_test, pred2)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y1_test, pred2)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y1_test, pred2)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, pred2)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "xgb_classifier.fit(x1_train, y1_train)\n",
    "\n",
    "xgb_pred = xgb_classifier.predict(x1_test)\n",
    "\n",
    "accuracy2 = accuracy_score(y1_test, xgb_pred)\n",
    "print(\"Accuracy: \", accuracy2*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y1_test, xgb_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y1_test, xgb_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y1_test, xgb_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y1_test, xgb_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y1_test, xgb_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, xgb_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "gb_classifier.fit(x1_train, y1_train)\n",
    "gb_pred = gb_classifier.predict(x1_test)\n",
    "\n",
    "accuracy3 = accuracy_score(y1_test, gb_pred)\n",
    "print(\"Accuracy: \", accuracy3*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y1_test, gb_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y1_test, gb_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y1_test, gb_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y1_test, gb_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y1_test, gb_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, gb_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_classifier = AdaBoostClassifier()\n",
    "ada_classifier.fit(x1_train, y1_train)\n",
    "ada_pred = ada_classifier.predict(x1_test)\n",
    "\n",
    "accuracy4 = accuracy_score(y1_test, ada_pred)\n",
    "print(\"Accuracy: \", accuracy4*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y1_test, ada_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y1_test, ada_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y1_test, ada_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y1_test, ada_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y1_test, ada_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, ada_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr_classifier.fit(x1_train, y1_train)\n",
    "lr_pred = lr_classifier.predict(x1_test)\n",
    "\n",
    "accuracy5 = accuracy_score(y1_test, lr_pred)\n",
    "print(\"Accuracy: \", accuracy5*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y1_test, lr_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y1_test, lr_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y1_test, lr_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y1_test, lr_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y1_test, lr_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, lr_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nn_classifier = KNeighborsClassifier()\n",
    "nn_classifier.fit(x1_train, y1_train)\n",
    "nn_pred = nn_classifier.predict(x1_test)\n",
    "\n",
    "accuracy5 = accuracy_score(y1_test, nn_pred)\n",
    "print(\"Accuracy: \", accuracy5*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y1_test, nn_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y1_test, nn_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y1_test, nn_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y1_test, nn_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y1_test, nn_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, nn_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_classifier = svm.SVC()\n",
    "svm_classifier.fit(x1_train, y1_train)\n",
    "svm_pred= svm_classifier.predict(x1_test)\n",
    "\n",
    "accuracy5 = accuracy_score(y1_test, svm_pred)\n",
    "print(\"Accuracy: \", accuracy5*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y1_test, svm_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y1_test, svm_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y1_test, svm_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y1_test, svm_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y1_test, svm_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, svm_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
