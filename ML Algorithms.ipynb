{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('heart_2020_cleaned.csv')\n",
    "dataset.head()\n",
    "\n",
    "# number of data in the dataset\n",
    "print(\"Number of data in the dataset: \", len(dataset.index))\n",
    "print(dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.duplicated().any():      # checking for duplicate data\n",
    "    dataset.drop_duplicates(inplace=True)       # removing the duplicates\n",
    "    print(\"Number of data after removing duplicates: \", dataset.shape[0])\n",
    "else: \n",
    "    print(\"No duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.isnull().values.any():    # checking for null data\n",
    "    print(dataset.isnull().sum())\n",
    "    dataset.dropna()        # removing the null values\n",
    "    print(\"Number of data after removing null values: \", dataset.shape[0])\n",
    "else:\n",
    "    print(\"No null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_target_data = dataset['HeartDisease'].value_counts()      # checking the number of 'yes' and 'no' in the label\n",
    "print(no_of_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We haven't encoded BMI, sleeptime, MentalHealth and PhysicalHealth\n",
    "# They are already integers and floats\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(dataset['HeartDisease'])\n",
    "dataset['HeartDisease']=label_encoder.transform(dataset['HeartDisease'])\n",
    "\n",
    "label_encoder.fit(dataset['Smoking'])\n",
    "dataset['Smoking']=label_encoder.transform(dataset['Smoking'])\n",
    "\n",
    "label_encoder.fit(dataset['AlcoholDrinking'])\n",
    "dataset['AlcoholDrinking']=label_encoder.transform(dataset['AlcoholDrinking'])\n",
    "\n",
    "label_encoder.fit(dataset['Stroke'])\n",
    "dataset['Stroke']=label_encoder.transform(dataset['Stroke'])\n",
    "\n",
    "label_encoder.fit(dataset['DiffWalking'])\n",
    "dataset['DiffWalking']=label_encoder.transform(dataset['DiffWalking'])\n",
    "\n",
    "label_encoder.fit(dataset['Sex'])\n",
    "dataset['Sex']=label_encoder.transform(dataset['Sex'])\n",
    "\n",
    "label_encoder.fit(dataset['AgeCategory'])\n",
    "dataset['AgeCategory']=label_encoder.transform(dataset['AgeCategory'])\n",
    "\n",
    "label_encoder.fit(dataset['Race'])\n",
    "dataset['Race']=label_encoder.transform(dataset['Race'])\n",
    "\n",
    "label_encoder.fit(dataset['Diabetic'])\n",
    "dataset['Diabetic']=label_encoder.transform(dataset['Diabetic'])\n",
    "\n",
    "label_encoder.fit(dataset['PhysicalActivity'])\n",
    "dataset['PhysicalActivity']=label_encoder.transform(dataset['PhysicalActivity'])\n",
    "\n",
    "label_encoder.fit(dataset['GenHealth'])\n",
    "dataset['GenHealth']=label_encoder.transform(dataset['GenHealth'])\n",
    "\n",
    "label_encoder.fit(dataset['Asthma'])\n",
    "dataset['Asthma']=label_encoder.transform(dataset['Asthma'])\n",
    "\n",
    "label_encoder.fit(dataset['KidneyDisease'])\n",
    "dataset['KidneyDisease']=label_encoder.transform(dataset['KidneyDisease'])\n",
    "\n",
    "label_encoder.fit(dataset['SkinCancer'])\n",
    "dataset['SkinCancer']=label_encoder.transform(dataset['SkinCancer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset after encoding the labels\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize= (12,12))\n",
    "sns.heatmap(dataset.corr(),annot = True,cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining features and label\n",
    "x = dataset.drop([\"HeartDisease\"], axis = 1).values     # feature\n",
    "y = dataset[\"HeartDisease\"].values      # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state= 42)\n",
    "\n",
    "print(\"Total number of data: \", x.shape[0])\n",
    "print(\"Total number of train data: \", x_train.shape[0])\n",
    "print(\"Total number of test data: \", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "x_train = robust_scaler.fit_transform(x_train)\n",
    "x_test = robust_scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "d_tree = tree.DecisionTreeClassifier()\n",
    "d_tree.fit(x_train, y_train)\n",
    "d_tree_pred = d_tree.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, d_tree_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "print(classification_report(y_test, d_tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_tree = tree.DecisionTreeClassifier()\n",
    "# d_tree_param = {\"criterion\": ['gini', 'entropy'],\n",
    "#                 \"max_depth\": range(3,10),\n",
    "#                 \"min_samples_split\": range(1,8),\n",
    "#                 \"min_samples_leaf\": range(1,5)\n",
    "#                 }\n",
    "# d_tree_grid_search = GridSearchCV(estimator=d_tree, param_grid=d_tree_param, cv = 5)\n",
    "# d_tree_grid_search.fit(x_train, y_train)\n",
    "# best_params = d_tree_grid_search.best_params_\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tree = tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                     max_depth = 5, \n",
    "                                     min_samples_split = 2,\n",
    "                                     min_samples_leaf = 1)\n",
    "d_tree.fit(x_train, y_train)\n",
    "d_tree_pred = d_tree.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, d_tree_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, d_tree_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, d_tree_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y_test, d_tree_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, d_tree_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y_test, d_tree_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, d_tree_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "pred = classifier.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier()\n",
    "\n",
    "# param_grid = {'n_estimators': [50, 75, 100, 125], \n",
    "#               'max_depth': [5, 6, 7, 8, 9, 10], \n",
    "#               'max_features': [5, 6, 7, 8]}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = classifier, param_grid = param_grid, cv = 5) \n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# best_parameters = grid_search.best_params_\n",
    "# print(\"Best Hyperparameters: \", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, \n",
    "                                     max_depth = 10, \n",
    "                                     max_features = 5)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "pred = classifier.predict(x_test)\n",
    "\n",
    "accuracy1 = accuracy_score(y_test, pred)\n",
    "print(\"Random Forest results after hyperparameter finetuning:\")\n",
    "print(\"Accuracy: \", accuracy1*100, \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y_test, pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "\n",
    "xgb_pred = xgb_classifier.predict(x_test)\n",
    "\n",
    "accuracy2 = accuracy_score(y_test, xgb_pred)\n",
    "print(\"Accuracy: \", accuracy2*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_classifier = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "# param_xgb = {\n",
    "#     'max_depth': range(2, 10, 1),\n",
    "#     'n_estimators': range(60, 220, 40),\n",
    "#     'learning_rate': [0.1, 0.01, 0.05]}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = xgb_classifier, param_grid = param_xgb, cv = 5) \n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# best_parameters = grid_search.best_params_\n",
    "# print(\"Best Hyperparameters: \", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(n_estimators = 140, \n",
    "                                     max_depth = 4, \n",
    "                                     learning_rate = 0.1,\n",
    "                                     objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "xgb_pred = xgb_classifier.predict(x_test)\n",
    "\n",
    "accuracy2 = accuracy_score(y_test, xgb_pred)\n",
    "print(\"XGBoost results after hyperparameter finetuning:\")\n",
    "print(\"Accuracy: \", accuracy2*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, xgb_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, xgb_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y_test, xgb_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, xgb_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y_test, xgb_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, xgb_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "gb_classifier.fit(x_train, y_train)\n",
    "gb_pred = gb_classifier.predict(x_test)\n",
    "\n",
    "accuracy3 = accuracy_score(y_test, gb_pred)\n",
    "print(\"Accuracy: \", accuracy3*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# param_gb = {\n",
    "#     \"n_estimators\":[5,50,250,500],\n",
    "#     \"max_depth\":[1,3,5,7,9],\n",
    "#     \"learning_rate\":[0.01,0.1,1]}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = gb_classifier, param_grid = param_gb, cv = 5) \n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# best_parameters = grid_search.best_params_\n",
    "# print(\"Best Hyperparameters: \", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_classifier = GradientBoostingClassifier(n_estimators = 50, \n",
    "                                     max_depth = 5, \n",
    "                                     learning_rate = 0.1)\n",
    "\n",
    "gb_classifier.fit(x_train, y_train)\n",
    "gb_pred = gb_classifier.predict(x_test)\n",
    "\n",
    "accuracy3 = accuracy_score(y_test, gb_pred)\n",
    "print(\"Gradient Boost results after hyperparameter finetuning:\")\n",
    "print(\"Accuracy: \", accuracy3*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, gb_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, gb_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y_test, gb_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, gb_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y_test, gb_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, gb_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_classifier = AdaBoostClassifier()\n",
    "ada_classifier.fit(x_train, y_train)\n",
    "ada_pred = ada_classifier.predict(x_test)\n",
    "\n",
    "accuracy4 = accuracy_score(y_test, ada_pred)\n",
    "print(\"Accuracy: \", accuracy4*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada_classifier = AdaBoostClassifier()\n",
    "\n",
    "# param_ada = {\n",
    "#     \"n_estimators\":[5,50,100,250],\n",
    "#     \"learning_rate\":[0.01,0.1,1]}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = ada_classifier, param_grid = param_ada, cv = 5) \n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# best_parameters = grid_search.best_params_\n",
    "# print(\"Best Hyperparameters: \", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_classifier = AdaBoostClassifier(n_estimators = 100, \n",
    "                                     learning_rate = 1)\n",
    "\n",
    "ada_classifier.fit(x_train, y_train)\n",
    "ada_pred = ada_classifier.predict(x_test)\n",
    "\n",
    "accuracy5 = accuracy_score(y_test, ada_pred)\n",
    "print(\"AdaBoost results after hyperparameter finetuning:\")\n",
    "print(\"Accuracy: \", accuracy5*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, ada_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, ada_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y_test, ada_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, ada_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y_test, ada_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ada_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_classifier = svm.SVC()\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "svm_pred= svm_classifier.predict(x_test)\n",
    "\n",
    "accuracy5 = accuracy_score(y_test, svm_pred)\n",
    "print(\"Accuracy: \", accuracy5*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, svm_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, svm_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y_test, svm_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, svm_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y_test, svm_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svm_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_classifier = svm.SVC()\n",
    "\n",
    "# param_svm = {\n",
    "#     \"kernel\":['linear', 'rbf'],\n",
    "#     \"C\":[1, 3, 5, 8], \n",
    "#     \"gamma\":[0.01, 0.05, 0.1]}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = svm_classifier, param_grid = param_svm, cv = 5) \n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# best_parameters = grid_search.best_params_\n",
    "# print(\"Best Hyperparameters: \", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_classifier = svm.SVC(kernel = best_parameters['kernel'], \n",
    "#                         C = best_parameters['C'],\n",
    "#                         gamma = best_parameters['gamma'])\n",
    "\n",
    "# svm_classifier.fit(x_train, y_train)\n",
    "# svm_pred = svm_classifier.predict(x_test)\n",
    "\n",
    "# accuracy6 = accuracy_score(y_test, svm_pred)\n",
    "# print(\"SVM results after hyperparameter finetuning:\")\n",
    "# print(\"Accuracy: \", accuracy6*100, \"%\")\n",
    "\n",
    "# print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(x_train, y_train)\n",
    "lr_pred = lr_classifier.predict(x_test)\n",
    "\n",
    "accuracy7 = accuracy_score(y_test, lr_pred)\n",
    "print(\"Accuracy: \", accuracy7*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_classifier = LogisticRegression()\n",
    "\n",
    "# param_lr = {\"penalty\":['l1', 'l2'],}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = lr_classifier, param_grid = param_lr, cv = 5) \n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# best_parameters = grid_search.best_params_\n",
    "# print(\"Best Hyperparameters: \", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(penalty = 'l2')\n",
    "\n",
    "lr_classifier.fit(x_train, y_train)\n",
    "lr_pred = lr_classifier.predict(x_test)\n",
    "\n",
    "accuracy7 = accuracy_score(y_test, lr_pred)\n",
    "print(\"LR results after hyperparameter finetuning:\")\n",
    "print(\"Accuracy: \", accuracy7*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, lr_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, lr_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y_test, lr_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, lr_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y_test, lr_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lr_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nn_classifier = KNeighborsClassifier()\n",
    "nn_classifier.fit(x_train, y_train)\n",
    "nn_pred = nn_classifier.predict(x_test)\n",
    "\n",
    "accuracy8 = accuracy_score(y_test, nn_pred)\n",
    "print(\"Accuracy: \", accuracy8*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# param_nn = {\"n_neighbors\":[3, 5, 7],\n",
    "#             \"weights\": ['uniform', 'distance'],\n",
    "#             \"metric\": ['euclidean', 'l1', 'manhattan']}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = nn_classifier, param_grid = param_nn, cv = 5) \n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# best_parameters = grid_search.best_params_\n",
    "# print(\"Best Hyperparameters: \", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_classifier = KNeighborsClassifier(n_neighbors=7,\n",
    "                                     weights='uniform',\n",
    "                                     metric='l1')\n",
    "\n",
    "nn_classifier.fit(x_train, y_train)\n",
    "nn_pred = nn_classifier.predict(x_test)\n",
    "\n",
    "accuracy9 = accuracy_score(y_test, nn_pred)\n",
    "print(\"NN results after hyperparameter finetuning:\")\n",
    "print(\"Accuracy: \", accuracy9*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, nn_pred)\n",
    "\n",
    "plt.figure(figsize= (4,3))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['Heart Disease','Not Heart Disease'],\n",
    "            yticklabels=['Heart Disease','Not Heart Disease'])\n",
    "plt.ylabel('Prediction',fontsize=8)\n",
    "plt.xlabel('Actual',fontsize=8)\n",
    "plt.title('Confusion Matrix',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, nn_pred)\n",
    "print(\"Accuracy: \", accuracy*100, \"%\")\n",
    "\n",
    "# precision_score and recall_score \n",
    "precision = precision_score(y_test, nn_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, nn_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# roc_auc score\n",
    "roc_auc = roc_auc_score(y_test, nn_pred)\n",
    "print(\"AUC (Area under the curve) score: \", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, nn_pred)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
